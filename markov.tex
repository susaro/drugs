In order to be able to describe dynamical systems having a nondeterministic behaviour, we introduce Markov processes.
They are memoryless stochastic processes which are commonly used to model different kinds of real-world processes, [ ].
% in different areas.
Recently they have been applied a lot in the research area of modelling biomolecular systems, [ ].
%As those systems consists of many particles, the corresponding process acts on a enormous large space,
%adequate, feasible time scales. computations/simulations
As those systems are enormous large, it is very difficult to perform simulations on feasible time scales.
%dimension/complexity
%That is basically a model of the original process, which acts on a smaller (projected) space.
That is why a reduction of complexity is needed, [ ].
%large (cont.). important/relevant. (long-time behaviour..) 
The originally large process is projected onto a suitable subspace, while maintaining the most relevant dynamical properties of the process. We present such a reduced model, called ``Markov State Model''.
%and enables us to.. .

%In principle a Markov State Model is a reduced model of a large (maybe continuous) stochastic process %which is built by conveniently grouping/ clustering some states of the process. That is done by a Galerkin %projection.
%To describe that, we need at first some basic definitions of stochastic processes, especially Markov %processes and how their evolution can be described using the transfer operator.

%define/create. adequately/rigorously
In order to adequately define a Markov State Model, we need at first some basic definitions and properties of stochastic processes, especially Markov processes, and how their evolution in time can be described using the transfer operator. The actual dimension reduction of the process is realized by a Galerkin projection applied to the transfer operator. By that action, states of the original process are clustered conveniently.
%clustered/grouped
%such that .. properties/transition rates?.. are preserved..

%The notation of this chapter is mainly based on the book Metastability and Markov State Models in Molecular %Dynamics from Sch\"utte and Sarich\cite{schutte2013metastability} which gives a good overview over the %basic concepts of Markov State Models.

\section{Markov Process}
\label{sec:markov}

%The theory of (discrete) Markov chains is not necessarily a prerequisite to understand the definitions in this %chapter, but still useful to know (see..). Here we will present a generalized (continuous) concept of this topic, %so from time to time we will refer to the special case of a Markov chain to exemplify/illustrate our %results/notions/definitions.

%being a special type?
We introduce Markov processes, which are a special type of stochastic processes and a generalization of the well-known Markov chains. Markov chains were defined as a memoryless process acting on a finite state space and evolving in discrete time, a behaviour that can be described by a stochastic matrix.
%complex,extensive
For general Markov processes, both these properties can be continuous and thus, we need some more extensive formulations and tools in order to describe such processes respectively their time-evolution rigorously.

\subsubsection*{Transition function}

%We suppose that the basics of probability theory (see ...) are known.

%From now on
We will denote by $E := (E,\Sigma)$ a \textit{measurable space}, that is a set $E$ with some $\sigma$-algebra $\Sigma$ defined on it. The triple $\Omega := (\Omega, \mathcal{A}, \Prob)$ will be a \textit{probability space}, that is a
%measure space with a probability measure
measurable space with a probability measure $\Prob$ defined on it; for detailed information about these basic measure theoretic notations, see Bogachev\cite[chapter~1]{bogachev2007measure}.
% Borel sigma-algebra on r is the smalles sigma-algebra of r which contains all open sets

A \textit{random variable} $X: \Omega \rightarrow E$ is a \textit{measurable function} from a probability space $\Omega$ into a measurable space $E$, meaning that preimages of measurable sets in $E$ are measurable in $\Omega$:
\begin{equation*}
A \in \Sigma \Rightarrow X^{-1}(A) \in \mathcal{A}.
\end{equation*}
Then the probability measure $\Prob$ of $\Omega$ induces a canonical probability measure on $E$, by
%which will also be denoted by
%denoted by $\mu$, by
\begin{equation*}
\mu(A) := \Prob(X \in A) := \Prob(X^{-1}(A))
\end{equation*}
for all $A \in \Sigma$, called the \textit{distribution} of $X$, see {\O}ksendal\cite[Section 2.1]{oksendal2003stochastic}.
%The measure mu is called..

\begin{defi}[Stochastic Process]
A family $\markov$ of random variables $X_t : \Omega \rightarrow E$ on some index set $\T$ is called
a \textit{stochastic process} on a state space $E$.
\end{defi}

In the following, we consider stochastic process on real state spaces $E \subset \R^d, d \in \N$, equipped with the Borel-$\sigma$-algebra $\Sigma = \B(E)$. \marginpar{?}
%special case/type
In order to introduce Markov processes as a special type of stochastic processes, we need a
%object, tool, definition, concept
tool to describe the time evolution or propagation of a process. This can be done using the transition function which describes the propagation of the distribution functions of a stochastic process.

\begin{defi}[Transition function]
% \marginpar{transfer operator: propagation of distributions?}
A function $p: \T \times E \times \Sigma \rightarrow [0,1]$ is a \textit{transition function} if it fulfills the following properties:
\begin{enumerate}
\item $x \mapsto p(t,x,A)$ is measurable on $E$ for all $t\in\T$ and $A\in\Sigma$,
\item $A \mapsto p(t,x,A)$ is a probability measure for all $t\in\T$ and $x\in E$,
\item $p(0,x,E \setminus x) = 0$ for all $x \in E$,
\item the Chapman-Kolmogorov equation
\marginpar{?}
\begin{equation}
\label{eq:chapman}
p(t+s,x,A) = \int_E p(t,x,\diff z)p(s,z,A).
\end{equation}
holds for all $t,s\in\T, x\in E$ and $A\in\Sigma$.
\end{enumerate}
\end{defi}
%reasonable (measurable)
In this definition, the first three properties ensure that we get reasonable results and that the process can only be in one state at the same time.
From the Chapman-Kolmogorov equation \eqref{eq:chapman}, it follows that the transition function $p(t,x,A)$ can be considered as the probability  to get into a certain subset $A$ in a time interval $t$ starting from a point $x$.
That means that we can describe the time evolution of a stochastic process by a transition function.
%(time discrete, finite state space)
In particular, the transition matrix of a Markov chain is a special case of the transition function since it fulfills the above properties.
%assuming we have a $1$-step transition matrix and $t=1$.
%but also for any other timesteps matrices

\subsubsection*{Markov Process}
%class/case/type
%Now we can define Markov processes, a special type of stochastic processes.
With the aid of a transition function, we are able to define Markov processes.

\begin{defi}[Markov Process]
A stochastic process $\markov$ on a state space $E$ is a \textit{Markov process} if its transition function fulfills the equation
\begin{equation}
\label{eq:markov_process_def}
p(t,x,A) = \Prob (X_{t+s} \in A \mid X_s =x).
\end{equation}
for all $s,t \in \T$, $x \in E$ and $A \in \Sigma$. If that probability is independent from $s$, then the Markov process is called \textit{time-homogeneous}.
\end{defi}

\clearpage
We are especially interested in time-homogeneous processes, which will be presumed from now on.
As we can see from the definition, all possible transition probabilities are given and hence, the time evolution of a Markov process is completely described by its transition function.
Thus a Markov process is uniquely determined by its transition function and an initial distribution $\mu$.
%last known state/current state/actual state. independent of its history
It is a process that has ``no memory'' in the sense that only the last known state of the process has an influence on the future of the process, as we can see on the right side of \eqref{eq:markov_process_def}.
%sequences evolving in time which remember their past trajectory only through its most recent value! (meyn)
%critical aspect:it is forgetful about all but its immediate past

%lag time

Indeed, there is a one-to-one relation between transition functions and Markov processes, i.e. every homogeneous Markov process defines a transition function and vice versa, see Meyn and Tweedie\cite[Section 3.4]{meyn1993}.
%A Markov process is uniquely determined by its transition function and an initial distribution $\mu$.
%bzw: Chapman-Kolm (m time steps) -> Markov property (1 time step) = special case of CK
% Markov prop -> Chapman-Kolm by using delta function
The beginning of a Markov process $X_t$ with the transition function $p$ fulfills
\begin{equation}
%Nielsen MA p.37 Theorem 2
\label{eq:beginning_markov}
\Prob_\mu (X_0 \in A , X_t \in B) = \int_A p(t,x,B) \mu(\diff x)
\end{equation}
for any $A,B \in \Sigma$, where $\Prob_\mu$ indicates that $X_0 \sim \mu$, or equivalently $\mu(A) = \Prob(X_0 \in A)$.
\\

The transition function for a Markov process plays the same role as the transition matrix for a Markov chain; it propagates its distributions. \marginpar{?}
If for the transition function we choose $t=1$ and transitions into one-elementic subsets, then the transition function corresponds to the $1$-step transition matrix $[p_{ij}]= P \in \R^{n \times n}$ of a Markov chain.
Having introduced the notion of Markov processes, we can define some important properties and give some examples.

\subsubsection*{Invariant Measure}

\begin{defi}[Invariant measure]
Let $\markov$ be a Markov process. The probability measure $\mu$ is \textit{invariant} with respect to $\markov$ if for all $t\in\T$ and $A \in \Sigma$ we have
\begin{equation*}
\int_E p(t,x,A) \mu(\diff x) = \mu(A).
\end{equation*}
\end{defi}
%If an invariant measure is additionally a probability measure, then it is also a stationary measure. (Nielsen p.18/19+28)
In other words, a measure is invariant with respect to a Markov process if the probability to \textbf{be} in any subset of the state space is the same as the probability to \textbf{get} into that subset by the evolution of the Markov process for any fixed transition time.

\subsubsection*{Ergodicity}
The long-time behaviour of stochastic processes can be described using ergodicity.
\begin{defi}[ergodic process]
Let $(X_t)_{t\in\T}$ be a Markov process with invariant probability measure $\mu$. Then $(X_t)_{t\in\T}$ is \textit{ergodic} with respect to $\mu$ if for all functions $u: E \rightarrow \R$ with $\int_E |u| \mu(\diff x) < \infty$
we have
% the following equation holds:
\begin{equation*}
\lim_{T \rightarrow \infty} \frac{1}{T} \int_0^T u(X_t) \diff t = \int_E u(x) \mu(\diff x).
%= \E_\mu[u].
\end{equation*}
for almost all initial values $X_0=x_0$.
\end{defi}
In that sense, a Markov process is ergodic if its time average is the same as its average over the probability space.
%known in the field of thermodynamics as its ensemble average.
In an ergodic process, the state of the process after a long time is nearly independent of its initial state.

%We will only consider/analyze ergodic Markov processes, i.e. processes that are \marginpar{only discrete case?} %irreducible
%\marginpar{only one eigenvalue 1} and aperiodic (no periodic behaviour).

\subsubsection*{Reversibility}

Reversibility describes the invariance of a process with respect to time-reversal. 
%A very useful property of Markov processes is reversibility.
In the next section, we will see that an operator describing a reversible process yields some very favorable properties, which makes it easy to analyze such a process.

\begin{defi}[reversible process]
Let $\markov$ be a Markov process with invariant probability measure $\mu$. Then $(X_t)_{t\in\T}$ is \textit{reversible} with respect to $\mu$ if
\begin{equation*}
\int_A p(t,x,B) \mu(\diff x) = \int_B p(t,x,A) \mu(\diff x)
\end{equation*}
for all $t \in \T$ and $A,B \in \Sigma$. If $\mu$ is unique, then $X_t$ is simply called \textit{reversible}.
%if process fulfills this definition for a distribution mu, then mu is invariant measure
\end{defi}

%reverse direction/transition
For a reversible process, the probability to get from any subset $A$ to another subset $B$ in a fixed time is the same as the probability for the reverse transition in the same time span.
Thus this definition implies that the process keeps the same probability law even if its movement is considered backwards in time.

If the stochastic transition function is absolutely continuous with respect to $\mu$, then reversibility corresponds to $p(t,x,y)=p(t,y,x)$ for all $t \in \T$ and $\mu$-almost all $x,y  \in E$.

\subsubsection*{Example: Markov Chain}

%To illustrate the previous definitions on an easy example, we set them in relation to the familiar/well-known special case of \textit{Markov Chains}.

Let $\markov$ be a Markov chain on discrete time $\T = \mathbb{N}$ and finite state space $E = \{1,\dots,n\}$.
Since we consider $1$-step transitions, the associated transition function is given by $p(x,y) := p(1,x,y)$ and corresponds to the entries of the \textit{transition matrix} $P\in \R^{n \times n}$, that is
\begin{equation*}
P_{xy} = p(x,y) = \Prob(X_1 = y \mid X_0 = x).
\end{equation*}
%probability, initial distribution
The propagation of a probability distribution $v_0 \in \R^n$ in the state space can be written as $v_1^T = v_0^T P$, where $v_0^T$ denotes the transposed vector of $v_0$.
%equilibrium distribution = stat. distribution if process converges to it indep. of start distribution
The invariant measure is given by the stationary distribution $\pi \in \R^n$, a normalized positive vector satisfying $\pi^T = \pi^T P$.
If $P$ is irreducible, such an eigenvector exists due to Perron-Frobenius theorem\cite[P7.3.5]{golub1996} and in this case, the corresponding eigenvalue $1$ is simple.

Reversibility of a Markov chain can be characterized by the \textit{detailed balance condition}
\begin{equation*}
\pi_i \cdot \Prob(X_1 = j \mid X_0 = i) = \pi_j \cdot \Prob(X_1 = i \mid X_0 = j) \ \ \ \forall i,j \in E.
\end{equation*}
%Then reversibility is equivalent to the detailed balance equation
A more compact way to write this equation uses the diagonal matrix $D = \textrm{diag}(\pi_1,\dots,\pi_n)$.
Then a Markov chain is reversible if and only if its transition matrix $P$ fulfills
\begin{equation}
\label{eq:detailed}
DP = P^T D.
\end{equation}
%measure/determine
This notation will be useful later in order to measure the degree of nonreversibility of a process.