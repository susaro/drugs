\section{Galerkin Projection}
%interpret $P_Q$ as a projected transfer operator
\label{sec:galerkin}

%Until now
%high-dimensional Markov processes
So far we considered Markov processes on very large, possibly continuous, state spaces.
For many applications, simulations of a given process are needed in order to obtain informations about the corresponding system. But computations on \textbf{large} state spaces require an enormous amount of computation power and time. On larger state spaces, the computation effort increases exponentially fast, see ``curse of dimension''. \marginpar{see ..}
%To avoid this
%In order to reduce the computation effort,
Therefore we are interested in reducing the number of states in order to make computations on reasonable time-scales feasible.
%practicable. concept/tool
Such a reducing can for instance be done by ``grouping similar states together''. In this section, we develop a mathematical concept which enables us to create such a reduced model.

%which shall inherit the most important properties of our original process. This can be done by a Galerkin projection/discretization.

\subsubsection*{Galerkin Projection}
%for Galerkin projection (ansatz space) and/or/= membership functions to apply PCCA+
%At first, we need to chose an appropriate/ desired/ favoured/ convenient ansatz space
The first step in order to create a finite process is to determine a convenient finite state space $D \subset L^2(\mu)$.
%adopt/select; approach/idea
For this purpose, we choose a partition of unity as a basis, which is a generalization of a set of characteristic functions.
That more general idea gives us more flexibility for later applications.
%flexibility/possibilities/options
The relevance of the choice of a partition of unity for the projection will be clarified in section \ref{sec:fuzzy}.

\begin{defi}[Partition of Unity]
A family of measurable functions  $\{ \cfam \}: E \rightarrow [0,1]$ in $L^2(\mu)$ is called a \textit{partition of unity} if the following two conditions are fulfilled:
%\marginpar{$\chi_i: E \rightarrow [0,1]$?}
\begin{enumerate}
\item The $\chi_i$ are non-negative and  linear independent.
%pairwise indep?
\item $\sum_{i=1}^n \chi_i(x) = 1$ for all $x \in E$.
%\marginpar{a.e.??}
\end{enumerate}
\end{defi}

\begin{defi}[Galerkin Projection]
Let $\{ \cfam \}$ be a partition of unity, $D = \mathrm{span}\{\cfam\}$ the associated finite-dimensional ansatz space and $\hat{S} \in \R^{n\times n}$ with $\hat{S}_{kj} = \langle \chi_k, \chi_j \rangle_\mu$. The \textit{Galerkin projection} onto $D$ is defined by $G: L^2 (\mu) \rightarrow D$ via
\begin{equation}
\label{eq:galerkin_def}
G f = \sum_{k,j=1}^n \hat{S}^{-1}(k,j) \langle \chi_k, f \rangle_\mu \chi_j.
\end{equation}
\end{defi}

The matrix $\hat{S}$ is invertible since it is the Gramian matrix of linear independent functions.
In the easy case that the $\{\cfam\}$ are the characteristic functions $\{\eins_{A_1},\dots,\eins_{A_n}\}$ belonging to a full partition $\{A_1,\dots,A_n\}$, equation \eqref{eq:galerkin_def} becomes
%\marginpar{``weighted'' orthogonal projection?}
\begin{equation*}
G f = \sum_{k=1}^n \frac{1}{\mu(A_k)} \langle \chi_k, f \rangle_\mu \chi_k,
\end{equation*}
since the $\chi_i$ are orthogonal which means that $\chi_k \chi_j = 1$ if $j=k$ and $0$ otherwise.
A Galerkin projection can be applied on the transfer operator of a Markov process as well.

\begin{defi}[Projected Transfer Operator]
Let $\Pcal := \Pcal^t$ be the transfer operator of a Markov process on a state space $E$ with unique invariant measure $\mu$, $\{ \cfam \}$ be a partition of unity and $G$ the Galerkin projection onto the associated subspace $D$. Then an operator of the form
\begin{equation*}
G \Pcal G: L^2 (\mu) \rightarrow D
\end{equation*}
is called \textit{projected transfer operator} and we abbreviate it by $G(\Pcal)$.
\end{defi}

\subsubsection*{Matrix Representation}

%As we are interested
Since we are interested in transitions inside of the projected space, we want to propagate $n$-dimensional vectors by the projected transfer operator. For this reason, we consider the projection of the restricted transfer operator $G\Pcal_{\mid D} : D \rightarrow D$, which will be denoted by $G(\Pcal)$ as well.
%use joint notation G(P) for simplicity
%\marginpar{GPG same as $GP_D$?}

%remind/remember/recall. described/represented. reference?
We recall that every linear map between finite-dimensional vector spaces can be represented by a matrix which is determined by chosen bases.
%why linear?
Accordingly, we can write the projected transfer operator as a $n \times n$-matrix in the following useful way.

\begin{thm}[Sarich \cite{[Theorem 1]sarich2011projected}]
%sarich p.45
%nielsen p. 39
\label{thm:galerkin}
Let $\Pcal$ be the transfer operator of a Markov process, $\{ \cfam \}$ a partition of unity and $G(\Pcal)$ the Galerkin projection of the transfer operator onto the associated subspace.
%\marginpar{ansatz space vs subspace}
Then $G(\Pcal)$ has a matrix representation
%Galerkin projection onto $D$ has a matrix representation
%is defined by $Q: L^2 (\mu) \rightarrow D$ by
\begin{equation*}
P_c = T S\inv,
\end{equation*}
where
\begin{equation}
\label{eq:massmatrix}
S_{kj} =  \frac{\hat{S}(k,j)}{\langle \chi_k, \eins \rangle_\mu}
= \frac{\langle \chi_k, \chi_j \rangle_\mu}{\langle \chi_k, \eins \rangle_\mu}
\end{equation}
and
\begin{equation*}
%T_{kj} = \frac{\langle \Pcal \chi_k, \chi_j \rangle_\mu}{\langle \chi_k, \eins \rangle_\mu}.
T_{kj} = \frac{\langle \chi_j, \Pcal \chi_k \rangle_\mu}{\langle \chi_k, \eins \rangle_\mu}.
\end{equation*}
%Both $S$ and $T$ are $n \times n$-matrices.
\end{thm}

\begin{proof}
Remember that $P_c$ is a (left) matrix representation of $G(\Pcal)$ with respect to a basis $\{\pfam\}$ of $D$ if for any function $f : D \rightarrow D$ with
\begin{equation}
\label{eq:representation0}
f = \sum_{i=1}^n \alpha_i\psi_i \ \textrm{ and } \ G(\Pcal) f = \sum_{i=1}^n \beta_i\psi_i
\end{equation}
it holds that
\begin{equation}
\label{eq:representation1}
(\afam)P_c = (\bfam).
\end{equation}
%want to show
We assume that \eqref{eq:representation0} is true and we aim to show \eqref{eq:representation1}.
For that purpose, we choose a basis $\{\pfam\}$ of $D$ with
\begin{equation}
\label{eq:representation3}
\psi_k = \frac{\chi_k}{\langle \chi_k, \eins \rangle_\mu}.
\end{equation}
As $G(\Pcal)$ is a linear map, we have $G(\Pcal)f = \sum \alpha_i G(\Pcal) \psi_i$. We exploit this fact, as well as the definitions of the Galerkin projection and the basis to compute
%given basis
\begin{align*}
G(\Pcal) f & \  =  \sum_{k=1}^n \alpha_k G(\Pcal) \psi_k \\
 & \stackrel{ \eqref{eq:galerkin_def}}{=}
    \sum_{k,l,j=1}^n \alpha_k \hat{S}\inv (j,l) \langle \chi_j, \Pcal \psi_k \rangle_\mu \chi_l \\
 & \stackrel{\eqref{eq:representation3}}{=}
     \sum_{k,l,j=1}^n \alpha_k \hat{S}\inv (j,l) \langle \chi_j, \Pcal \psi_k \rangle_\mu \langle \chi_l, 		      \eins \rangle_\mu \psi_l \\
 & \stackrel{\eqref{eq:representation0}}{=}
     \sum_{l=1}^n \beta_l\psi_l.
\end{align*}
%we see that we can express
Comparing the coefficients of the last two equations, we can express $\beta_l$ as
\begin{align*}
\beta_l & = \sum_{k,j=1}^n  \alpha_k \hat{S}^{-1} (j,l) \langle \chi_l, \eins \rangle_\mu \langle \chi_j, \Pcal \psi_k \rangle_\mu  \\
 & = \sum_{k=1}^n \alpha_k \underbrace{\sum_{j=1}^n \hat{S}^{-1} (j,l) \langle \chi_l, \eins \rangle_\mu \frac{\langle \chi_j, \Pcal \chi_k \rangle_\mu}{\langle \chi_k, \eins \rangle_\mu}}_{\stackrel{!}{=} (P_c)_{kl}}. \numberthis \label{eq:representation2}
\end{align*}
The underbraced term \textbf{should} be equal to $(P_c)_{kl}$ because we wish that \eqref{eq:representation1} is fulfilled.
Thus, we compute the $(k,l)$-th entry of $P_c = TS\inv$, employing the fact that $(S\inv)_{jl} = (\hat{S}\inv)_{jl} \langle \chi_l, \eins \rangle$, as
%via matrix multiplication as
\marginpar{notation!}
\begin{align*}
(TS^{-1})_{kl} & = \sum_{j=1}^n T_{kj} (S\inv)_{jl} \\
 &  = \sum_{j=1}^n \frac{\langle \chi_j, \Pcal \chi_k \rangle}{\langle \chi_k, \eins \rangle}
(\hat{S}\inv)_{jl} \langle \chi_l, \eins \rangle
\end{align*}
%find out
and discover that it is equal to the underbraced term in \eqref{eq:representation2}.
Hence, \eqref{eq:representation1} is true and therefore $P_c$ is the requested matrix representation of $G(\Pcal)$.
%Hence, we obtain the desired result \eqref{eq:representation1} and therefore $P_c$ is the requested matrix representation of the Galerkin projection $G(\Pcal)$.
\end{proof}
%Nielsen p.40

\begin{thm}
\label{thm:galerkin_stochastic}
The matrices $S$ and $T$ from theorem \ref{thm:galerkin} are stochastic.
\end{thm}
\begin{proof}
In order to be stochastic, each row must sum up to $1$. We exploit the partition of unity property $\sum_j \chi_j = 1$ for all $j$ and the aforementioned properties $\Pcal  \eins =  \eins$ and $\Pcal^{*} \eins = \eins$ of a transfer operator respectively its adjoint:
\begin{equation*}
\sum_{j=1}^n S_{kj}
= \frac{\langle \chi_k, \sum_j\chi_j \rangle_\mu}{\langle \chi_k, \eins \rangle_\mu}
= \frac{\langle \chi_k, \mathbb{1} \rangle_\mu}{\langle \chi_k, \eins \rangle_\mu} =1,
\end{equation*}
\begin{equation*}
\sum_{j=1}^n T_{kj}
= \frac{\langle \sum_j \chi_j, \Pcal \chi_k \rangle_\mu}{\langle \chi_k, \eins \rangle_\mu}
= \frac{\langle \eins, \Pcal \chi_k \rangle_\mu}{\langle \chi_k, \eins \rangle_\mu}
= \frac{\langle \Pcal^{*} \eins, \chi_k \rangle_\mu}{\langle \chi_k, \eins \rangle_\mu}
= 1.
\end{equation*}
Non-negativity follows from the non-negativity of the $\{\cfam \}$.
\end{proof}
Since $S$ and $T$ are both stochastic matrices, they have the constant vector $\eins_D$ as right eigenvector to the eigenvalue $1$. It implies that the same holds for $P_c$, that is
%\marginpar{can propagate prob. distr. (keeping norm)}
its rows sum up to $1$ and thus the product $TS\inv$ is \textbf{at least pseudostochastic}. But nonnegativity is not assured since inverting $S$ can provoke negative entries. The non-negativity depends on the choice of the partition of unity. As we will see, there are examples such that $TS\inv$ is a stochastic matrix.
%provoke/evoke/produce/cause
%Interpretation: as norm is preserved, distributions can be propagated

\begin{thm}
\label{thm:lefteigenvector}
The matrix representation $P_c$ from theorem \ref{thm:galerkin} has the left eigenvector $\hat{\mu} \in D$ with the entries
%left-eigenvector = invariant measure = stationary distribution/ measure of $P_c$
\begin{equation*}
\hat{\mu}_j = \langle \eins, \chi_j \rangle_\mu = \int_E \chi_j(x) \mu(\diff x).
\end{equation*}
\end{thm}

\begin{proof}
We observe that $\hat{\mu}^T S = \hat{\mu}^T$ and $\hat{\mu}^T T = \hat{\mu}^T$ since
\begin{equation*}
(\hat{\mu}^T S)_j = \sum_{k=1}^n  \langle \eins, \chi_k \rangle_\mu \frac{\langle \chi_k, \chi_j \rangle_\mu}{\langle \chi_k, \eins \rangle_\mu}
= \langle \eins, \chi_j \rangle_\mu = \hat{\mu}_j
\end{equation*}
and
\begin{equation*}
(\hat{\mu}^T T)_j = \sum_{k=1}^n \langle \chi_j, \Pcal \chi_k \rangle_\mu = \langle \chi_j, \Pcal \eins \rangle_\mu
=  \langle \chi_j, \eins \rangle_\mu = \hat{\mu}_j.
\end{equation*}
We can deduce that $\hat{\mu}^T P_c = \hat{\mu}^T TS^{-1} = \hat{\mu}^T S^{-1} = \hat{\mu}^T SS^{-1} = \hat{\mu}^T$.
\end{proof}

\begin{rem}
We remark that in theorem \ref{thm:galerkin} we stated the matrix representation of the projection of a \textbf{forward} transfer operator.
%As the backward operator is the adjoint operator of the propagator, \marginpar{nop}
For a backward operator, the proof works almost simultaneously and yields the matrix representation $P_c = S^{-1}T$, where $T = \frac{\langle \chi, \Pcal \chi \rangle_\mu}{\langle \chi, \eins \rangle_\mu}$. In that case, $P_c$ is a right matrix representation and propagates sets instead of probability distributions.
%This result and its properties are computed completely analogously to the previous proofs.
%$P_c$ propagates probability distributions, while $P_c^T$ propagates sets(?).
\end{rem}
%norm needs not be conserves for set propagation???
%left or right multiplication of vectors???

This matrix decomposition will be of particular interest for out later investigations.

\subsubsection*{Example: Full Partition Discretization}
%Nielsen Diss p. 42
Let $A_1, \dots, A_n$ be a partition of the state space $E$, i.e. they are pairwise disjoint sets such that $\cup A_i = E$. We consider the family of the corresponding characteristic functions
\begin{equation*}
\chi_i (x) = \eins_{A_i} (x).
\end{equation*}
Since they are orthogonal, the matrix $S$ is the identity matrix and therefore, the matrix representation of the Galerkin projection is $P_c = T$.
We can compute it by combining
\begin{equation*}
\langle \Pcal \eins_{A_i}, \eins_{A_j} \rangle_\mu = \int_{A_j} (\Pcal \eins_{A_i})(x) \mu(\diff x)
\stackrel{\eqref{eq:forward}}{=}    \int_{A_i} p(t,x,A_j) \mu(\diff x)
\stackrel{\eqref{eq:beginning_markov}}{=}        \Prob_\mu(X_t \in A_j, X_0 \in A_i)
\end{equation*}
and
\begin{equation*}
\langle \eins_{A_i}, \eins \rangle_\mu = \int_E \eins_{A_i} (x) \mu(\diff x) = \mu(A_i)
= \Prob_\mu (X_0 \in A_i).
\end{equation*}
%The resulting matrix representation is a stochastic matrix consisting of the transition probabilities between the partition sets and its entries are given by
The entries of the resulting matrix representation are given by
\begin{equation*}
T_{ij} = \frac{\langle \Pcal \eins_{A_i}, \eins_{A_j} \rangle_\mu}{\langle \eins_{A_i}, \eins \rangle_\mu}
= \Prob_\mu(X_t \in A_j \mid X_0 \in A_i).
\end{equation*}
%\marginpar{ref to 1-1-rel?},T
Thus, $P_c$ represents a Markov chain whose state space consists of the partition sets $A_i$, e.g. each $A_i$ is a ``macro state'' of the projected process.
The stationary distribution of this Markov chain $P_c$ is just the projection of the invariant measure $\mu$ onto $D$.
\\

%This example makes clear that blablabla
For a full partition discretization, the matrix $S$ is a diagonal matrix. If we choose a partition of unity that is \textit{close} to a full partition, i.e. we choose \textit{almost characteristic functions}, then the matrix $S$ is not diagonal, but close to that. We will later see the consequences of that fact regarding to the examination of the rebinding effect.
%more overlap: more rebinding?
%examination/investigations

\subsubsection*{Properties of Galerkin Projection} \marginpar{what about $S\inv T$?}

As the matrix representation of a projected transfer operator is in general \textbf{not} a stochastic matrix, and stochastic matrices are in a one-to-one relation with Markov chains,
 %which is equivalent to being the transition matrix of a Markov chain (see ..),
we can immediately deduce that the process can lose its Markovianity by projecting it onto a subspace.
This possible loss of Markovianity is certainly a really undesirable effect. But before examining that later in section \ref{sec:recrossing}, let us now first analyze further properties of the matrix representation $P_c$.
\\

We already know that the matrices $S$ and $T$ from Theorem \ref{thm:galerkin} are stochastic matrices. This leads to some good properties of $P_c$:

\begin{itemize}
\item The eigenvalue $\lambda = 1$ of $P_c$ has the associated right-eigenvector $e = (1,\dots,1)^T$ and left-eigenvector $\hat{\mu}^T$ from theorem \ref{thm:lefteigenvector}.
\item If $\Pcal$ is self-adjoint in $L^2(\mu)$, then $G(\Pcal)$ as well. Then the matrices $S$ and $T$ are self-adjoint with respect to the discrete scalar product \marginpar{$P_c$ self-adj.?}
\marginpar{$\langle Av,w \rangle = \langle v, Aw \rangle$}
\begin{equation*}
\langle u, v \rangle_{\hat{\mu}} = \sum_{i=1}^n u_i v_i \hat{\mu}_i.
\end{equation*}
\marginpar{eigenv. of S, T real}
%and in $[-1,1]$
Since self-adjointness of the operator is equivalent to reversibility of the corresponding process, see theorem \ref{thm:selfadjoint_reversible}, detailed balance equation (e.g. $\hat{\mu}_k T_{kl} = \hat{\mu}_l T_{lk}$ for all $k,l =1,\dots, n$) is fulfilled for both $S$ and $T$.
\item If the transfer operator has a simple and dominant eigenvalue $1$ and the continuous part of the spectrum is bounded away from the discrete part, then the process is irreducible and aperiodic which is inherited by the matrix $T$. In particular $T$ has the simple and dominant eigenvalue $\lambda=1$ which is the only eigenvalue with $|\lambda|=1$ and the discrete invariant density $\hat{\mu}$ is the unique invariant density of $T$.
\item As seen in the last example, a full-partition projection yields the transition matrix $P_c=T$ of a Markov chain describing transitions between the partition sets.
\end{itemize}

%Summarizing,
%maintains/inherits/keeps
The discretization inherits many important properties of the original process.

%\subsubsection*{Summary, Conclusion}
\subsubsection*{Interpretation}
%p.65
%The discretization of the propagator can be interpreted as a \textit{coarse graining} procedure.
%in the sense that ...
%transfer operator/propagator.
\marginpar{?}
%Interpretation of $S$ as a ``stochastic mass matrix'' and $T$ as a ``coupling matrix'' (transition matrix).

The matrix $T$ is defined in terms of $\Pcal$, while $S$ is defined only by the partition of unity $\chi_i$. Thus, the matrix $T$ represents the propagation of the process and is also called ``coupling matrix'' as it describes how the clustered subsets are interacting.
%clusters

If the partition of unity consists of characteristic functions, i.e. the $\chi_i$ are orthogonal, then the behaviour of the clustered process is completely determined by $T$. The larger the deviation from the $\chi_i$ to being orthogonal, the larger the deviation of the clustered process from $T$.
This argumentation will be elaborated in chapter \ref{chap:meta}.
%amplified/elaborated

\subsubsection*{Projected infinitesimal generator}

The Galerkin projection of an infinitesimal generator yields a similar matrix representation as the transfer operator. It can also be written as the product of two stochastic matrices, one of them being the inverted mass matrix of the partition of unity functions.

\begin{thm}[Sch\"utte and Sarich\cite{schutte2013metastability}]
Let $\Qcal: L^2(\mu) \rightarrow L^2(\mu)$ be a generator of a semigroup of transfer operators with unique invariant measure $\mu$ and satisfying $\Qcal\eins = 0$.
Let $\chi$ be a partition of unity with a projection $G$ onto the associated subspace spanned by $\chi$.
Then the projected generator $G(\Qcal)$ has the matrix representation $Q_c = RS\inv$ with the stochastic mass matrix $S$ from \eqref{eq:massmatrix} and
\begin{equation*}
R(k,j) = \frac{\langle \Qcal\chi_k, \chi_j \rangle_\mu}{\langle \chi_k, \eins \rangle_\mu}
\end{equation*}
The eigenvalue problem of $Q$ is equivalent to the generalized eigenvalue problem $Ru = \Lambda Su$.
For both $Q$ and $R$ the largest eigenvalue is $\lambda = 0$. The associated right eigenvector is $e=(1,\dots,1)^T$, the associated left eigenvector is $\hat{\mu}^T$ from theorem \ref{thm:lefteigenvector}.
\end{thm}
%For the other properties see...
The proof is similar to theorem \ref{thm:galerkin} and equivalently, the matrix representation of the infinitesimal generator corresponding to the backward transfer operator is given by $Q_c = S\inv R$ with $R = \frac{\langle \chi_k, \Qcal \chi_j \rangle_\mu}{\langle \chi_k, \eins \rangle_\mu}$.
\\

%possibilities/options. transfer operator propagator/generator. discretization/projection
There are obviously many possible Galerkin projections of a given transfer operator. We showed the example of a full-partition discretization, which results in a very easy matrix representation. But as arbitrary partitions of unity $\cfam$ are allowed for a Galerkin projection, there will be more variational results.
%transfer operator respectively infinitesimal generator 
%\marginpar{propagator, generator}, desirable/favorable
%maintains, represents
In chapter \ref{chap:meta} we are going to see which choice of $\chi$ results in a good discretization in the sense that it represents the correct long-time behaviour of the process in terms of so called \textit{metastability}.
%using so called