\section{Molecular Kinetics as a Projection}
\label{sec:projection}
%\marginpar{Mol. Kinetics Weber p.10}

%In this section, 
We will deduce a rigorous description of molecular systems, in particular receptor-ligand-systems, basically by embedding the mathematical concepts from chapter \ref{chap:markov} into an adequate physical context.
% deduce/develop/derive
When considering such systems, we can distinguish between two points of view: we will see how we can get from the \textit{microsopic} or \textit{atomistic} to a  \textit{macroscopic} scale by a projection.

%\todo{which model? Hamiltonian ($\Gamma$)? Hamiltonian w/ randomized momentum ($\Omega$ w/ any momentum)? Langevin? Diffusion Dynamics?}
%ensemble,subensemble,observable

\subsubsection*{Micro States}
%Boltzmann distribution = example for a canonical ensemble
A micro state of a molecular system with $N$ atoms can be represented in a $6N$-dimensional \textit{phase space} $\Gamma = \Omega \times \Rdrei$, consisting of the \textit{configuration space} $\Omega = \Rdrei$ and the \textit{momentum space} $\Rdrei$. In the following, we consider systems in \textit{thermodynamical equilibrium}.
%assume equilibrium
%\R_{+}
One possible model is given by the \textit{Boltzmann distribution} $\pi: \Omega \times \R^{3N} \rightarrow \R$, a
%\marginpar{prob. dens. fct.?}
probability distribution assigning to each micro state a probability depending on its energy and temperature,
see McQuarrie\cite{mcquarrie2000}. It can be expressed as
% in the form
\begin{equation}
%\pi(q,p) \propto \exp{(-\beta H(q,p))},
\pi(q,p) = \frac{1}{Z} \exp{(-\beta H(q,p))},
\end{equation}
%The distribution shows that states with lower energy will always have a higher probability of being occupied %than the states with higher energy
where $\beta = 1/ (k_BT)$ is the inverse of the temperature $T$ multiplied with the Boltzmann constant $k_B$
%\marginpar{$\diff$?}
and $Z= \int_\Gamma \exp{-\beta H(q,p)} \diff(q,p)$ is the normalization factor. The Hamilton function denoted by $H$ is given by $H(q,p) = K(p)+V(q)$, the sum of the kinetic energy $K(p)$ and the potential energy $V(q)$.
Thus, the Boltzmann distribution $\pi$ can be decomposed into $\pi = \pi_p \pi_q$,
%as?
\begin{equation*}
%\pi(q,p) \propto \exp{(-\beta K(p))} \exp{(-\beta V(q))}, \underbrace{a}{b}
\pi(q,p) =  \underbrace{\frac{1}{Z_p} \exp{(-\beta K(p))}}_{\pi_p} \cdot
\underbrace{\frac{1}{Z_q} \exp{(-\beta V(q))}}_{\pi_q},
\end{equation*}
where $\pi_p: \R^{3N}  \rightarrow \R$ is the probability density function of the kinetic part in the momentum space $\R^{3N}$ and $\pi_q: \Omega \rightarrow \R$ is the probability density function of the potential part in the configuration space $\Omega$.
%conformations/metastable sets
%being objects in configuration space
As we are interested in examining conformations, that are objects in configuration space, we will restrict ourselves to $\Omega$, see Huisinga\cite{huisinga2001metastability}:
%Huisinga p.12
\begin{quote}
``A conformation $C \subset \Omega$ will be identified with the particular metastable sub-ensemble $\mu_{C \times \R^{3N}}$ corresponding to the particular subset $C \times \R^{3N} \subset \Gamma$. Hence, for every position $q \in C$, the conformation contains all states with $q \in \Omega$ and arbitrary $p \in \R^{3N}$.''
\end{quote}
%conformations/metastable sets
In this sense, conformations contain no information on momenta and are determined in configuration space only. We consider a reduced model with the \textit{reduced density} $\pi_q = \int_{\R^{3N}} \pi(q,p) \diff p$.

%\todo{difference conformation vs. metastable set}

\subsubsection*{Macro States via Membership Functions}

%As the phase space and even the configuration space; by grouping/clustering; average/ordinary
As the configuration space of an average molecular system is very large, we aim to reveal the underlying  discrete Markov State Model by clustering a collection of the micro states having the same or similar values in one observable. Such a collection of micro states will be called a \textit{macro state}.
For instance, that could be the states ``bound'' or ``unbound'' for a simple receptor-ligand system. %states/observables
%\marginpar{entropic inf.?}
%and entropic information?
%Macro states need not be distinct sets.

We apply the function-based fuzzy clustering method presented in section \ref{sec:fuzzy}.
%\marginpar{overlap = good?}
We define macro states as \textbf{overlapping} partial densities, which can be identified as membership functions $\cfam : \Omega \rightarrow [0,1]$.
%using membership functions which can have certain overlap.
They form a partition of unity, that is
\begin{equation*}
\sum_{i=1}^n \chi_i(q) = 1.
\end{equation*}
%So: membership function "$=$" macro state? NOP
By grouping micro states, the corresponding macro states yield statistical weights
\begin{equation}
%pi_q?
%here: continuous process and thus \eins instead of e
\label{eq:statistical_weights}
w_i = \langle \chi_i, \eins \rangle_\pi := \int_\Omega \chi_i(q) \pi_q(q) \diff q,
\end{equation}
%The statistical weight $w_i$ corresponds
corresponding to the ``probability for the system to \textbf{be} in conformation $\chi_i$''. %Weber Habil p.2
%(metastable) macro state $i$

\subsubsection*{Transfer Operator}
%Weber habil p.14
Each micro state $(q,p) \in \Gamma$ determines a \textit{probability density function} $\Psi^{-\tau}(\cdot \mid (q,p))$ describing the possible evolutions of the system in configuration space $\Omega$ in time $\tau$, having started at the initial state $(q,p)$.
%For instance, $\Psi^{-\tau}(\tilde{q} \mid (q,p))$ is the probability to get to $\tilde{q} \in \Omega$ starting %from $(q,p) \in \Gamma$.
%For a given micro state $(q,p) \in \Gamma$, we define the probability density function $\Psi^{-\tau}%(\tilde{q} \mid (q,p))$ as the probability of the system to get to $\tilde{q} \in \Omega$ in time %(step) $\tau$, after being started in $(q,p)$.
Weber\cite{weber2011subspace} defines a transfer operator $\Pcal (\tau): L^{1,2}_{\pi_q}(\Omega) \rightarrow L^{1,2}_{\pi_q}(\Omega)$ acting on membership functions via %for the propagation of membership functions via
\begin{equation}
\label{eq:transferoperator}
\Pcal(\tau)f(q) = \int_\Rdrei \left( \int_\Omega f(\tilde{q})\Psi^{-r}(\tilde{q}\mid (q,p)) \diff \tilde{q} \right) \pi_p(p)\diff p.
\end{equation}
In this definition, the density function $\Psi^{-\tau}(\cdot \mid (q,p))$ can be interpreted as a transition function as defined in section \ref{sec:markov}.
We notice that this operator corresponds to a \textbf{backward} transfer operator as introduced in section \ref{sec:transfer}. %\marginpar{?}

It is a \textit{generalized} transfer operator in the sense that it includes deterministic as well as stochastic dynamical models. In order to describe deterministic dynamics, the density function $\Psi^{-\tau}$ has to be chosen as a Dirac delta function, since an initial state $(q(0),p(0))$ determines exactly the future states in configuration space.
\\

%Weber Habil p.28 Markov Operator!
%\marginpar{adjoint operator}
% (corresponding)
It is important to remark that the transfer operator $\Pcal(\tau)$ also defines a projected Markov operator $\overline{\Pcal} (\tau)$ acting in configuration space $\Omega$, see Weber\cite{weber2011subspace}, by
\begin{equation}
\label{eq:markov_operator}
\overline{\Pcal} (\tau) = \pi_q \circ \Pcal(\tau) \circ (\pi_q)^{-1},
\end{equation}
which propagates density functions and thus corresponds to a \textbf{forward} transfer operator.
The previous equation shows that the space of membership functions is connected to the space of density functions by multiplication with $\pi_q$.
%Weber Habil.
%but, though, yet
We will keep that relation in mind, though just use $\Pcal$ in the following.
%Pcal = propagates sets/membership fct; Pcalbar = propagates densities.
\\

As $\Pcal(\tau)$ in \eqref{eq:transferoperator} propagates \textbf{membership functions}, stationarity is characterized by the equation $e=\Pcal(\tau)e$ for the constant function $e = 1$ in $\Omega$. For the Markov operator $\overline{\Pcal} (\tau) $ in \eqref{eq:markov_operator} propagating \textbf{densities}, stationarity can be characterized by $\pi$ = $\overline{\Pcal} (\tau) \pi$, where $\pi$ is the Boltzmann density. %\marginpar{Boltzmann dens. = dist.?}
These two operators are adjoint operators. This can also be seen by the fact that a discretization of $\Pcal(\tau)$ results in a matrix $P_c$, while a discretization of $\overline{\Pcal} (\tau)$ will result in the transposed matrix $P_c^T$.

%\subsubsection*{Maybe: Properties of transfer operator for reversible Processes}
%\subsubsection*{Spectrum of Transfer Operator for reversible processes}
%Detailed Balance
%\begin{equation}
%\label{eq:detailed_balance}
%\pi_q (\tilde{q}) \cdot \int_{\R^{3N}} \Psi^{-r}(q \mid (\tilde{q},p)) \pi_p(p)\diff p 
%	= \pi_q(q) \cdot \int_{\R^{3N}} \Psi^{-r}(\tilde{q}\mid (q,p)) \pi_p(p)\diff p
%\end{equation}

\subsubsection*{Markov State Model for reversible Processes}

For now, we consider a reversible process. In that case, the corresponding transfer operator $\Pcal := \Pcal(\tau)$ is \textbf{self-adjoint} with respect to $\pi_q$ and thus has a real spectrum with $\sigma(\Pcal) \subset [-1,1]$. %see theorem \ref{thm:spectrum_operator}.
%(since $\Vert \Pcal f \Vert_{\pi_q} \leq \Vert f \Vert_{\pi_q}$).
%The reason for that can be understood by theorem \ref{thm:selfadjoint_reversible}, but we additionally need to consider the projection onto the configuration space. A rigorous proof is found in Weber\cite[theorem 1]{weber2011subspace}.
%If the process is non-reversible, then this operator is not self-adjoint and hence can possess complex %eigenvalues. We will handle this case in section \ref{sec:rebinding_nonreversible}.
In order to apply the spectral approach from section \ref{sec:spectral}, we assume that the \textbf{discrete spectrum} of the transfer operator $\Pcal$ has $n$ \textbf{dominant eigenvalues} $1 = \lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_n$ which are all close to $1$ and bounded away from the essential spectrum.
The corresponding dominant eigenfunctions are denoted by $\Xcal = \{ \Xcal_1,\dots, \Xcal_n\}$ and therefore the eigenvalue problem is given by $\Pcal \Xcal = \Xcal \Lambda$, with the eigenvalue matrix $\Lambda = \mathrm{diag}(\lambda_1, \dots, \lambda_n)$.

As we know from chapter \ref{chap:meta}, the number of metastable sets of a Markov process is determined by the number of dominant eigenvalues; thus we are going to create a Markov State Model on $n$ states.
%Since each of the $n$ dominant eigenvalues of the transfer operator corresponds to a metastable set (no? %because membership fct instead of set?)...
%Using the dominant spectrum of the transfer operator, we want to create a discrete Markov State Model on %$n$ states.
%macro states/metastable sets/conformations
The state space of this model should consist of the conformations of the molecular system in consideration and its transition behaviour should be described by a $n\times n$-transition matrix $P_c := P_c(\tau)$.
%by/via; i.e. row-stochastic matrix???
In order to obtain this discrete matrix  from the continuous operator $\Pcal$, we need at first to determine the size and shape of the membership functions  $\chi_i$.
As described in section \ref{sec:fuzzy}, this can be done by computing a linear combination of the dominant eigenfunctions via
\begin{equation}
\label{eq:pcca_membership}
\chi_j(q) = \sum_{i=1}^N A_{ij}\Xcal_i(q), \ j=1,\dots,n,
\end{equation}
where $A= \{A_{ij}\}_{i,j=1,\dots,n}$ is the solution of PCCA+ (convex maximization problem).
As a linear combination of (orthogonal) eigenfunctions, the membership functions $\chi_i$ might have an overlap; they are not orthogonal, but span the same subspace as the $\Xcal$.
\\

%\subsubsection*{Galerkin Projection}
%With these membership functions, 
A Markov State Model is created by applying the Galerkin discretization
%$\Pcal(\tau)$ is projected onto a finite Markov State Model $P_c(\tau)$ by applying the Galerkin discretization
\begin{equation}
%\label{eq:galerkin}
P_c(\tau) = G(\Pcal(\tau)) = (\langle \chi, \chi \rangle_\pi)\inv (\langle \chi, \Pcal(\tau) \chi \rangle_\pi).
\end{equation}
%$(\Pcal(\tau))^k = (P(\tau))^k$
Theorem \ref{thm:iteration_error} implies that there is no discretization error under this projection, that is we have $G(\Pcal^k)$ = $(P_c)^k$. In particular, Markovianity is preserved.
%Theorem \ref{thm:iteration_error} is true for arbitrary regular transformations $A$ of $\Xcal$, so in particular it holds for the solution computed by PCCA+.
%Since the projection represents the correct dynamical long-time behaviour of the process, the matrix $P_c(\tau)$ is the correct Markov State Model.
We can use the matrix representation $P_c(\tau)=S^{-1}T$ from theorem \ref{thm:galerkin}. Then $S$ and $T$ are stochastic matrices with \marginpar{falschrum}
\begin{equation}
\label{eq:projection_presentation}
\begin{aligned}
T & = D^{-1} \langle \chi, \Pcal(\tau) \chi \rangle_\pi = D^{-1}A^T \Lambda A \ \ \textrm{ and } \\
S & = D^{-1} \langle \chi, \chi \rangle_\pi = D^{-1} A^T A,
\end{aligned}
\end{equation}
%\begin{equation}
%T = D^{-1} \langle \chi, \Pcal(\tau) \chi \rangle_\pi = D^{-1}A^T \Lambda A \ \textrm{ and } \ 
%S= D^{-1} \langle \chi, \chi \rangle_\pi = D^{-1} A^T A,
%\end{equation}
where $D= \mathrm{diag}(w_1,\dots,w_n)$ is the diagonal matrix of statistical weights from \eqref{eq:statistical_weights}.

\subsubsection*{Measuring the Rebinding Effect}

%habil p.34
%Even though $P_c(\tau)$ is the correct Markov State Model, it cannot be interpreted as a transition matrix, since the inverse matrix of $S$ is not necessarily stochastic.
%\marginpar{falsch!!!}
%$T = D^{-1} \langle \chi, \Pcal \chi \rangle$
We remember the interpretation of the matrix representation $P_c = S\inv T$ of the Markov State Model from section \ref{sec:fuzzy}.
The stochastic matrix $T$ determines the long-time behaviour of the clustered process, yet the Markov State Model differs from $T$ by
%he matrix $T$ however is a transition matrix. The difference between $P_c(\tau)$ and $T$ is given by
\begin{equation*}
S P_c(\tau) = T.
\end{equation*}
%disturbance,deviation
This ``deviation'' of the Markov State Model $P_c(\tau)$ from the stochastic matrix $T$ can be measured by the matrix $S$.
%If $S$ is equal to the identity matrix, then the Markov State Model is Markovian.
If $S$ is equal to the identity matrix, then the Markov State Model is solely determined by $T$.
%If $S$ is close to the identity matrix, then $P_c(\tau)$ is close to being a stochastic matrix.
If $S$ is close to the identity matrix, then $P_c(\tau)$ is close to $T$ and not strongly influenced by the overlap matrix $S$.
The more the matrix $S$ differs from the identity matrix, the more the Markov State Model $P_c(\tau)$ differs from the transition matrix $T$.
%Since for a matrix, being stochastic is equivalent to being the transition matrix of a Markov chain, the deviation of $P_c(\tau)$ from being stochastic can be interpreted as a violation of Markovianity and thus, of including some kind of memory.
This is due to the recrossing/rebinding events.
The larger this deviation, the larger the occuring memory effects.
Thus, the rebinding effect, introduced in section \ref{sec:rebinding} as a \textbf{memory effect} provoked by a projection, can be measured by the matrix $S$.

%corresponds to the results of our previous examinations. In section 1.4 ... In section 3.1
These observations correspond to our previous results from the examination of the recrossing effect in section \ref{sec:recrossing} and the qualitative description of the rebinding effect in section \ref{sec:rebinding}. \marginpar{?}
%characteristic functions
%We found out that a crisp clustering, i.e. a set based-clustering, leads to a model $P_c(\tau)$ which is Markovian, caused by the orthogonality of membership functions and thus $S$ being the identity matrix.
The more \textbf{overlap} occured in the membership functions, the more the matrix $S$ deviates from being the identity matrix (higher outer diagonal elements) and thereby includes more memory effects.

Thus, the rebinding effect can be measured by the trace of the matrix $S$, the sum of its diagonal elements. It can lie between $0$ (very much overlap) and $n$ (diagonal matrix). This approach has been introduced by Weber and Fackeldey\cite{weber2014} and will be used in the next chapter to detect the minimal rebinding effect included in a system.
\\


%The data we get for our computations is based on simulations. As a transition rate matrix can be detected %from these simulations (instead of transition matrix), we formulate the above result for transition rates %instead of transition probabilities.

%One further result/remark:
Since PCCA+ maximizes the crispness of $S$, we can assume that it induces a small rebinding effect. \marginpar{wrong?}

\subsubsection*{Infinitesimal Generator to transition rate matrix}

%consider/examine. get/obtain
Often it is more convenient to analyze transition rate matrices instead of transition matrices.
Thus, we consider the infinitesimal generator $\Qcal$, which is defined from $\Pcal(\tau)$ via
%In order to get a discrete transition rate matrix, we apply the Galerkin projection to the infinitesimal generator  induced by a given transfer operator.
%The transfer operator $\Pcal(\tau)$ defines a time-independent operator $\Qcal$ via
\begin{equation*}
\Qcal = \lim_{\tau \rightarrow 0} \frac{\Pcal(\tau)-\mathcal{I}}{\tau}.
\end{equation*}
%which is the infinitesimal generator of $\Pcal$.
Moreover, these two operators are related by \marginpar{Chapman}
\begin{equation*}
\Pcal(\tau) = \exp{(\tau\Qcal)}.
\end{equation*}
Weber\cite{weber2011subspace} shows that such an infinitesimal generator exists for a discretization in terms of membership functions.
Since the eigenfunctions of $\Qcal$ and $\Pcal$ are the same and their eigenvalues are related via $\exp{(\xi_i)} = \lambda_i$, we can apply the same Galerkin Projection for the infinitesimal generator as for the transfer operator.
%For the infinitesimal generator we can apply the same Galerkin Projection as for the transfer operator in
%\eqref{eq:galerkin}
We get a $n\times n$-rate matrix
\begin{equation}
\label{eq:galerkin_infinitesimal}
Q_c = A^{-1} \Xi A = (\langle \chi, \chi \rangle_\pi)\inv (\langle \chi, \Qcal \chi \rangle_\pi),
\end{equation}
where $\Xi$ is the diagonal matrix consisting of the $n$ leading eigenvalues $0 = \xi_1 > \xi_2 \geq \cdots \geq \xi_n$ of $\Qcal$ and $A$ is the transformation matrix of \eqref{eq:pcca_membership}, which analoguously transforms the eigenfunctions of $\Qcal$ into membership functions of the macro states.
The clustered matrix $Q_c$ can be interpreted as a transition rate matrix. \marginpar{?}
%\clearpage