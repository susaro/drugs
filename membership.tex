\section{Fuzzy Clustering}
\label{sec:fuzzy}

%(metastable decomposition induced by zeros of eigenfunctions)
The above considerations result in a metastable full decomposition of the state space, that is each state is assigned to exactly one of the partition sets.
We will see now, that there exist better solutions, considering/including the fact that transition regions can belong to several metastable conformations. So for this slightly more general approach,
%(already examined/investigated/analyzed in recent research, see  ...),
there may be some overlap in the assignment of states to metastable sets.

% a transition region can be %assigned to several macro states with different weight(?).

\subsubsection*{Set-based vs. Function-based Approach}
An intuitive approach to decompose the state space would be to determine a certain number of metastable sets which form a full partition of the state space, such that each state is assigned to exactly one of the metastable sets.
%each partition corresponds to one conformation/ metastable set.
The problem with that approach is that also the transition regions of the process would have to be assigned to one of these partition sets. But why would you assign a state in a transition region to one adjacent metastable set and not to the/one other? So such an assignment is not a rigoruous description of actual behaviour of the process.
%does not make so much sense.

Therefore, this \textit{set-based} or \textit{crisp approach} of decomposing the process has been replaced by the \textit{function-based} or \textit{fuzzy/soft approach}.
That means that each state of the process is assigned with a certain ``degree of membership'' ($\approx$ probability) to each metastable conformation.
That is reasonable, because a state in a transition region (local minimum) cannot be assigned to a single conformation. Thus, these clustering-functions should be ``overlapping''.
%allowing to assign a state to different conformation with certain probabilities/degree of membership

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.5\textwidth]{figures/fig_fuzzy_set.jpg}
 	\caption{Crisp vs Fuzzy Sets/ Clustering}
        \label{fig:fuzzy}
\end{figure}

%\subsubsection*{Fuzzy Sets}

Fuzzy sets are sets whose elements have degrees of membership.

%\begin{defi}[Fuzzy Set]
%A fuzzy set is a pair $(U,m)$ where $U$ is a set and $m:U \rightarrow [0,1]$ a membership function.
%\end{defi}

\subsubsection*{Membership functions}

%reversible
Assuming we have already determined that our process consists of $n$ metastable sets (by knowing its $n$ dominant eigenvalues).
%the number of metastable sets by 
%we are considering a stochastic process with $n$ dominant eigenvalues
We will follow the approach of Weber\cite{weber2006meshless} to define macro states as \textit{overlapping partial densities}. They can be identified by membership functions $\cfam : \X \rightarrow [0,1]$. Each state of the original state space shall be assigned to the different macro states with a certain \textit{degree of membership}.
%using membership functions (linear combinations of eigenfunctions). no??

\begin{defi}[Membership Function \cite{weber2006meshless}]
% (see \ref{sec:galerkin})
The functions $\cfam : \X \rightarrow [0,1]$ are called \textit{membership functions} if they fulfill
\begin{itemize}
\item $\chi_j(i) \geq 0$ $\forall i \in \X$ and $\forall j \in \{1,\dots,n\}$ (positivity),
\item $\sum_{j = 1}^n \chi_j(i) = 1$ $\forall i \in \X$ (partition of unity).
\end{itemize}

%For any set $\X$, a membership function on $\X$ is a function $m: \X \rightarrow [0,1]$.
%A membership function is a function $\chi_i : \X \rightarrow [0,1]$ \marginpar{part. of unity?}
%= partition of unity and nonnegative
\end{defi}

%finite state space: membership vectors (for PCCA+ since we can only apply it on finite processes?)
We can interpret membership functions as assigning each state $i$ to a cluster/conformation $j$ with a certain ``probability'' $\chi_j(i)$.
%or ``degree of membership''
For each conformation $j \in \{ 1 , \dots, n \}$, there is a membership function $\chi_j$, which determines the portion of the partial density w.r.t. the total density function. \marginpar{?}
These membership functions form a partition of unity, in order to sum up to the total density.
%some density function given? t.ex. Boltzmann distribution
%Weber Diss 10
\\

The previous example of a full-partition discretization corresponds to the choice of characteristic functions $\{ \eins_{A_1},\dots, \eins_{A_n} \}$ as membership functions. They are also called \textit{crisp} or \textit{hard} membership functions, whereas the general (overlapping) membership functions are denoted as \textit{fuzzy} or \textit{soft}.
%while/whereas
The \textit{crispness} of the clustering(?) can be measured by the matrix $S$.
Nonoverlapping membership functions (char. fcts.) yield an overlap matrix $S = D\inv \langle \chi, \chi \rangle$ equal to the unit matrix.
Overlapping membership functions yield a matrix with non-zero outer diagonal elements.
\\

Membership functions can be used to decompose the space into metastable sets/clusters. In this case, the assignment of a state to a metastable set must not be unique, but a state can belong to different metastable sets with certain degrees, which can be interpreted as kind of probabilities. \marginpar{?} That model takes into consideration
%takes into account
the existence of transitions regions which cannot be uniquely assigned to one macro state/metastable set.
%detect a 

%membership functions: continuous and overlapping

Since membership functions form a partition of unity, we can apply the Galerkin projection as defined in section \ref{sec:galerkin}.
If we choose the membership functions w.r.t. metastability, then we get a Markov State Model where each (macro) state is a metastable set of the original process. \marginpar{so far: membership fct indep. of metast. set}
%As membership functions are a partition of unity, we can 1) create a metastable decomposition (maybe with %overlap, so no partition) and 2) apply Galerkin Projection on it (partition of unity necessary).
\\

Individual eigenfunctions $\Xcal$ do not overlap since they are orthogonal. But the membership functions $\chi_j$ as linear combinations of the dominant eigenfunctions might have an overlap.
 \marginpar{?}
\begin{equation*}
P_c \chi_j \approx \chi_j \ \forall j=1,\dots,n.
\end{equation*}
\begin{equation*}
T\chi_j \approx S\chi_j \ \forall j=1,\dots,n.
\end{equation*}

From the definition, membership functions like $\chi_1 = \chi_2 = 0.5$ are possible, but not interesting. Instead, they are often chosen to be \textbf{close} to a characteristic function, like shown in figure \ref{fig:fuzzy}. That is reasonable, since it puts the emphasis of a conformation onto a certain region (high degree of membership) and maybe some adjacent parts (low degree of membership).
For this reason, they are also often called \textit{almost characteristic functions} in literature.

\subsubsection*{Statistical Weights} \marginpar{what for?}

For each macrostate we can define a statistical weight
\begin{equation*}
w_i = \langle \chi_i, \eins \rangle_\mu = \int_\X \chi_i(q) \diff \mu(q),
\end{equation*}
which describes the \textit{portion} of a membership function to the total density function. \marginpar{?}
$D = \mathrm{diag}(w_1,\dots,w_n)$ is the diagonal matrix of the statistical weights of the membership functions. Then $T=D^{-1} \langle \chi, \Pcal(\tau) \chi \rangle_\mu$, compare theorem \ref{thm:galerkin}.

\subsubsection*{Perron Cluster Analysis} \marginpar{finite/cont. vec./fct.}

The term \textit{Perron Cluster Analysis} denotes the objective of clustering a Markov process into metastable sets using the \textit{Perron eigenvalues} respective \textit{Perron eigenfunctions}, which means eigenvalues close to $1$ and the corresponding eigenfunctions.
Perron Cluster Analysis respectively its algorithmic implementation PCCA (\textit{Perron Cluster Cluster Analysis}) has been developed by Deuflhard et al\cite{deuflhard2000identification} which used the sign structure of the dominant eigenvalues of the transition matrix. \marginpar{set-based approach?}
%as what?
That approach has been improved by Deuflhard and Weber\cite{deuflhard2005robust}
%\marginpar{oder Weber und Galliat?}
who transformed the system of eigenvectors into a system of membership functions which results in a soft/fuzzy clustering of the state space of the original process; their algorithm is called PCCA+
(\textit{Robust Perron Cluster Analysis}).
%adapted
Originally, PCCA+ was formulated only for discrete Markov chains, but Weber\cite{weber2011subspace} extended it even on continuous processes.
%we present here the more general case for cont. processes with a given transfer operator
\\

%The  whole approach  is  called Perron  cluster  analysis, its agorithmic  realization is  abbreviated  as  PCCA  %(from  Perron  Cluster  Cluster  Analysis). \marginpar{PCCA+: robust}
%We need to choose/determine convenient/suitable membership functions s.t. the metastable decomposition %becomes as good as possible. Normed eigenfunctions are one possibility (no overlap). But better: linear %combinations of eigenfunctions. \marginpar{only for finite matrices? or also cont. operator w/ eigenfct.?}

%Let $\Xcal$ be the eigenvector matrix, \marginpar{eigenfct.?} i.e. the $i$-th column of $\Xcal$ is an %eigenvector corresponding to the eigenvalue $\lambda_i$. \marginpar{only dom. spec.}

We consider the set of dominant eigenvalues $\{\lfam\}$ with the corresponding set of eigenfunctions $\Xcal = \{\xfam\}$.
They fulfill the eigenvalue problem $\Pcal(\tau) \Xcal = \Xcal \Lambda$ of the transfer operator $\Pcal(\tau)$, where $\Lambda = \mathrm{diag}(\lfam)$.
The set of membership functions $\chi= \{\cfam \}$ can be built as a linear combination $\Xcal \Acal$ of the dominant eigenfunctions, that is
\begin{equation}
\chi_j(q) = \sum_{i=1}^n \Acal_{ij} \Xcal_i(q), \ j=1,\dots,n.
\end{equation}
Here,  $\Acal = \{\Acal_{ij}\}_{i,j=1,\dots,n} \in \R^{n\times n}$ is a real matrix which should be chosen in such a way that the resulting membership functions fulfill the required properties/constraints; i.e. positivity and partition of unity.
%Noe Lecture 4 (MSM 2015) 
There are infinitely many transformations $\Acal$ of the eigenvectors resulting in a soft membership matrix
$\chi$ satisfying the positivity and partition of unity constraints.
Consequently, we have to determine the transformation  $\Acal$ that satisfies some optimality condition.
The algorithm PCCA+ computes the matrix $\Acal$ as the solution of a convex maximization problem, see Weber\cite{weber2006meshless}. \marginpar{optimal metastability?}
%yields
%In order to fulfill the partition of unity property of each $\chi_i$, the matrix $A$ has to be chosen s.t. $\chi$ %is row-stochastic.

\marginpar{only conv. lin.comb.?}
Weber shows in \cite{weber2011subspace} that the for choice $\chi = \Xcal \Acal$ (any linear combination of the eigenfunctions $\Xcal$), the discretization error of the Galerkin projection vanishes and hence, diagram \ref{fig:diagram_transfer} commutes. In particular, such membership functions preserve the Markov property of the process.
%preserves the Markov property of the process. \marginpar{even: discretiz. error vanishes}
%Markovianity
\\

If we project a finite process, then the above formulation results in a \textit{membership vector matrix} $\chi$, which is the result of a linear combination of the \textit{eigenvector matrix}.

\subsubsection*{Measuring/Maximizing crispness of }
%Weber Diss 58
The \textit{crispness} of .. can be measured by the matrix $S$.

The metastability of a process is defined via the trace of $P_c$, but can also measured via its determinant.
\marginpar{see ...}
If the metastability is high, then $\det(P_c)$ is close to $1$. More clearly, by multiplication of the determinant we have
\begin{equation*}
\det(P_c) = \det(S) \det(\Lambda).
\end{equation*}
Thus, in order to increase the metastability of a system, both determinants need to be high. $\det(S)$ is maximized if the linear combination $\chi = \Xcal \Acal$ is as \textit{crisp} as possible.
%maximize $\det(S)$

%\subsubsection*{Iteration Error}

%Second largest eigenvalue results in the best decomposition and so on ...

%But Better: linear combination of eigenfunctions (might have an overlap) (membership functions, PCCA+).
%That will be described more in the next subsection 2.4 where, in aim to get a really good MSM, we try to %find the best possible decomposition (?) into metastable sets (? not a partition anymore) 

%Do we need PCCA+ to find the best(?) membership functions. And with the help of these membership %functions we can apply a Galerkin Projection?